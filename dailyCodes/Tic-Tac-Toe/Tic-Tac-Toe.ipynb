{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practice: Tic Tac Toe\n",
    "##### ***Jun 8***\n",
    "##### ***Charles Zhang***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment(State) Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.end = False\n",
    "        self.board_hash = None\n",
    "        self.player = 1    # default that p1 plays first\n",
    "    \n",
    "    def show_board(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n",
    "        \n",
    "    def winner(self):\n",
    "        \"\"\"\n",
    "        :return:1 if P1 wins, -1 if P2 wins, 0 if tie \n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results.append(np.sum(self.board[i, :]))\n",
    "        for i in range(BOARD_COLS):\n",
    "            results.append(np.sum(self.board[:, i]))\n",
    "        results.append(0)\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results[-1] += self.board[i, i]    \n",
    "        results.append(0)\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results[-1] += self.board[i, BOARD_ROWS - 1 - i]\n",
    "\n",
    "        for result in results:\n",
    "            if result == 3:\n",
    "                self.end = True\n",
    "                return 1\n",
    "            elif result == -3:\n",
    "                self.end = True\n",
    "                return -1\n",
    "        # for tie\n",
    "        sum = np.sum(np.abs(self.board))\n",
    "        if sum == BOARD_ROWS * BOARD_COLS:\n",
    "            self.end = True\n",
    "            return 0\n",
    "        self.end = False\n",
    "        return None\n",
    "    \n",
    "    # get available positions\n",
    "    def get_space(self):\n",
    "        space = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    space.append((i, j))  # need to be tuple\n",
    "        return space\n",
    "    \n",
    "    def give_reward(self):\n",
    "        winner = self.winner()\n",
    "        if winner == 1:\n",
    "            self.p1.feed_reward(1)\n",
    "            self.p2.feed_reward(0)\n",
    "        elif winner == -1:\n",
    "            self.p1.feed_reward(0)\n",
    "            self.p2.feed_reward(1)\n",
    "        else:\n",
    "            self.p1.feed_reward(0.1)    # can be changed for tie\n",
    "            self.p2.feed_reward(0.5)\n",
    "        \n",
    "    def play(self, player):\n",
    "        positions = self.get_space()\n",
    "        player_action = player.get_action(positions, self.board, self.player)\n",
    "        self.update_state(player_action)\n",
    "        board_hash = self.get_hash()\n",
    "        player.add_state(board_hash)\n",
    "        return self.winner()\n",
    "    \n",
    "    def play_against_self(self, rounds=5000):\n",
    "        for i in range(rounds):\n",
    "            if i%1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.end:\n",
    "                winner = self.play(self.p1)\n",
    "                if winner is not None:\n",
    "                    self.give_reward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "                else:\n",
    "                    winner = self.play(self.p2)\n",
    "                    if winner is not None:\n",
    "                        self.give_reward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "                \n",
    "    def play_against_human(self):\n",
    "        while not self.end:\n",
    "            positions = self.get_space()\n",
    "            p1_action = self.p1.get_action(positions, self.board, self.player)\n",
    "            self.update_state(p1_action)\n",
    "            self.show_board()\n",
    "            winner = self.winner()\n",
    "            if winner is not None:\n",
    "                if winner == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "            else:\n",
    "                positions = self.get_space()\n",
    "                p2_action = self.p2.get_action(positions)\n",
    "                self.update_state(p2_action)\n",
    "                self.show_board()\n",
    "                winner = self.winner()\n",
    "                if winner is not None:\n",
    "                    if winner == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "        \n",
    "    def update_state(self, position):\n",
    "        self.board[position] = self.player\n",
    "        self.player *= -1\n",
    "        \n",
    "    def get_hash(self):\n",
    "        self.board_hash = str(self.board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return self.board_hash    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.board_hash = None\n",
    "        self.end = False\n",
    "        self.player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Setting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ϵ-greedy method to balance between exploration and exploitation. If ϵ=0.3 , 70% of the time our agent will take greedy action, which is choosing action based on current estimation of states-value, and 30% of the time our agent will take random action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []  \n",
    "        self.alpha = 0.5  # learning rate\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 1 # discont factor, may be used in the future\n",
    "        self.states_value = {}  # state value dictionary\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_hash(board):\n",
    "        board_hash = str(board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return board_hash\n",
    "    \n",
    "    def get_action(self, positions, current_board, player_name):\n",
    "        \"\"\"\n",
    "        :param positions: available positions(actions) for now\n",
    "        :return: the best action considering the greedy move\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            idx = np.random.choice(len(positions))  # random action\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            max_value = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = player_name\n",
    "                next_board_hash = self.get_hash(next_board)\n",
    "                if self.states_value.get(next_board_hash) is None:\n",
    "                    value = 0 \n",
    "                else:\n",
    "                    value = self.states_value.get(next_board_hash)\n",
    "                if value >= max_value:\n",
    "                    max_value = value\n",
    "                    action = p\n",
    "        return action\n",
    "        \n",
    "    def feed_reward(self, reward):\n",
    "        # when the game ends, reversed to find st+1\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.alpha*(self.decay_gamma*reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def add_state(self, state):\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        \n",
    "    def save_policy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load_policy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def get_action(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "            \n",
    "    def add_state(self, state):\n",
    "        pass\n",
    "\n",
    "    def feed_reward(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Rounds 0\n",
      "Rounds 1000\n",
      "Rounds 2000\n",
      "Rounds 3000\n",
      "Rounds 4000\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "p1, p2 = Player(\"p1\"), Player(\"p2\")\n",
    "state = State(p1, p2)\n",
    "print(\"training...\")\n",
    "state.play_against_self()\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.save_policy()\n",
    "p2.save_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "Input your action row:1\n",
      "Input your action col:1\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "Input your action row:0\n",
      "Input your action col:0\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "Input your action row:2\n",
      "Input your action col:1\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "-------------\n",
      "| o | x |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "Input your action row:0\n",
      "Input your action col:2\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "tie!\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"computer\", exp_rate=0)\n",
    "p1.load_policy(\"policy_p1\")    # update the state values after learning\n",
    "p2 = HumanPlayer(\"human\")\n",
    "state = State(p1, p2)\n",
    "state.play_against_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "Input your action row:1\n",
      "Input your action col:1\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "Input your action row:0\n",
      "Input your action col:2\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "Input your action row:0\n",
      "Input your action col:0\n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x | x | x | \n",
      "-------------\n",
      "computer wins!\n"
     ]
    }
   ],
   "source": [
    "state.play_against_human()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**reference:**\n",
    "    \n",
    "http://incompleteideas.net/book/the-book-2nd.html\n",
    "    \n",
    "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction\n",
    "    \n",
    "https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
